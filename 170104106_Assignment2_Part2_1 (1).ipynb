{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170104106_Assignment2_Part2_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aTo6FyigWBx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import mean\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIN4jxPIgnpI",
        "outputId": "abafcac6-04d2-47c3-8d7d-01816ab1b6b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'drive/My Drive/SoftCom/training-a'\n",
        "a_csv = pd.read_csv('drive/My Drive/SoftCom/training-a.csv',low_memory=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "ot6iGypugvCN",
        "outputId": "fa8ea680-e04e-45c2-c07e-0d2c979dd10c"
      },
      "source": [
        "#a_csv = pd.read_csv('/content/drive/MyDrive/SoftCom/training-a.csv')\n",
        "a_csv.columns\n",
        "a_csv = a_csv.drop(columns=['original filename', 'scanid',\n",
        "       'database name original', 'contributing team', 'database name'])\n",
        "a_csv.iloc[:10, 0:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a00000.png</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a00001.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a00002.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a00003.png</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a00004.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>a00005.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>a00006.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a00007.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>a00008.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>a00009.png</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     filename  digit\n",
              "0  a00000.png      5\n",
              "1  a00001.png      3\n",
              "2  a00002.png      1\n",
              "3  a00003.png      7\n",
              "4  a00004.png      0\n",
              "5  a00005.png      4\n",
              "6  a00006.png      3\n",
              "7  a00007.png      0\n",
              "8  a00008.png      4\n",
              "9  a00009.png      9"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZexWoX5gyud"
      },
      "source": [
        "label_csv = a_csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDFIKv_bg8qc"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" + item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r74tkqQzhAAG",
        "outputId": "17fedf18-6862-49b1-8ceb-251e6c9eea71"
      },
      "source": [
        "mean = [0.5,]\n",
        "std = [0.5, ]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(180),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(180),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_data  = Dataset(label_csv, path, train_transform)\n",
        "test_data = Dataset(label_csv, path, test_transform)\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainig Samples:  19702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0J611xlhBUV"
      },
      "source": [
        "batch_size = 10\n",
        "num_iters = 40000\n",
        "input_dim = 180*180 \n",
        "num_hidden = 200  \n",
        "output_dim = 10\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "learning_rate = 0.01  # More power so we can learn faster! previously it was 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ciUquAhK89",
        "outputId": "1c7228b8-f4c8-49d7-a434-e4f9ceb5d3d0"
      },
      "source": [
        "import torch.nn as nn\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        \n",
        "        self.relu_1 = nn.ReLU()\n",
        " \n",
        "        \n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        \n",
        "        self.relu_2 = nn.ReLU()\n",
        " \n",
        "        \n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        \n",
        "        self.relu_3 = nn.ReLU()\n",
        " \n",
        "         \n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        \n",
        "        self.relu_4 = nn.ReLU()\n",
        " \n",
        "         \n",
        "        self.linear_5= nn.Linear(num_hidden, num_hidden)\n",
        "        \n",
        "        self.relu_5= nn.ReLU()\n",
        " \n",
        "         \n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        \n",
        "        self.relu_6 = nn.ReLU()\n",
        " \n",
        "        \n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        " \n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        " \n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_4(out)\n",
        " \n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        " \n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_6(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        " \n",
        "# INSTANTIATE MODEL CLASS\n",
        " \n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        " \n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        " \n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        " \n",
        "        images = images.view(-1, 180*180).to(device)\n",
        "        labels = labels.to(device)\n",
        " \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        " \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        " \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        " \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        " \n",
        "        iter += 1\n",
        " \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 180*180).to(device)\n",
        " \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        " \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        " \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        " \n",
        " \n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        " \n",
        "            accuracy = 100 * correct.item() / total\n",
        " \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. \\n Loss: {}.\\n  Accuracy: {}\\n'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. \n",
            " Loss: 2.2987451553344727.\n",
            "  Accuracy: 10.070043650390824\n",
            "\n",
            "Iteration: 1000. \n",
            " Loss: 2.299448251724243.\n",
            "  Accuracy: 10.070043650390824\n",
            "\n",
            "Iteration: 1500. \n",
            " Loss: 2.3128137588500977.\n",
            "  Accuracy: 10.237539336108009\n",
            "\n",
            "Iteration: 2000. \n",
            " Loss: 2.29634952545166.\n",
            "  Accuracy: 9.968531113592528\n",
            "\n",
            "Iteration: 2500. \n",
            " Loss: 2.3081936836242676.\n",
            "  Accuracy: 9.968531113592528\n",
            "\n",
            "Iteration: 3000. \n",
            " Loss: 2.2955334186553955.\n",
            "  Accuracy: 9.963455486752615\n",
            "\n",
            "Iteration: 3500. \n",
            " Loss: 2.281014919281006.\n",
            "  Accuracy: 10.070043650390824\n",
            "\n",
            "Iteration: 4000. \n",
            " Loss: 2.3068153858184814.\n",
            "  Accuracy: 9.968531113592528\n",
            "\n",
            "Iteration: 4500. \n",
            " Loss: 2.2423930168151855.\n",
            "  Accuracy: 9.963455486752615\n",
            "\n",
            "Iteration: 5000. \n",
            " Loss: 2.106621265411377.\n",
            "  Accuracy: 9.983757994112272\n",
            "\n",
            "Iteration: 5500. \n",
            " Loss: 2.3241522312164307.\n",
            "  Accuracy: 9.770581666835854\n",
            "\n",
            "Iteration: 6000. \n",
            " Loss: 1.949838399887085.\n",
            "  Accuracy: 9.917774845193382\n",
            "\n",
            "Iteration: 6500. \n",
            " Loss: 2.2078003883361816.\n",
            "  Accuracy: 10.97858085473556\n",
            "\n",
            "Iteration: 7000. \n",
            " Loss: 2.069056510925293.\n",
            "  Accuracy: 9.917774845193382\n",
            "\n",
            "Iteration: 7500. \n",
            " Loss: 3.3010668754577637.\n",
            "  Accuracy: 12.775352756065374\n",
            "\n",
            "Iteration: 8000. \n",
            " Loss: 2.099926471710205.\n",
            "  Accuracy: 11.541975433966094\n",
            "\n",
            "Iteration: 8500. \n",
            " Loss: 2.0618062019348145.\n",
            "  Accuracy: 11.831286163841234\n",
            "\n",
            "Iteration: 9000. \n",
            " Loss: 2.135094165802002.\n",
            "  Accuracy: 9.917774845193382\n",
            "\n",
            "Iteration: 9500. \n",
            " Loss: 2.0311460494995117.\n",
            "  Accuracy: 15.896863262612932\n",
            "\n",
            "Iteration: 10000. \n",
            " Loss: 1.9272615909576416.\n",
            "  Accuracy: 9.374682773322505\n",
            "\n",
            "Iteration: 10500. \n",
            " Loss: 2.344804525375366.\n",
            "  Accuracy: 11.404933509288398\n",
            "\n",
            "Iteration: 11000. \n",
            " Loss: 1.852961778640747.\n",
            "  Accuracy: 10.004060501471931\n",
            "\n",
            "Iteration: 11500. \n",
            " Loss: 2.285888195037842.\n",
            "  Accuracy: 10.278144350827327\n",
            "\n",
            "Iteration: 12000. \n",
            " Loss: 1.9125276803970337.\n",
            "  Accuracy: 9.968531113592528\n",
            "\n",
            "Iteration: 12500. \n",
            " Loss: 2.3160455226898193.\n",
            "  Accuracy: 11.455689777687544\n",
            "\n",
            "Iteration: 13000. \n",
            " Loss: 1.9149348735809326.\n",
            "  Accuracy: 11.53689980712618\n",
            "\n",
            "Iteration: 13500. \n",
            " Loss: 1.4427473545074463.\n",
            "  Accuracy: 10.085270530910567\n",
            "\n",
            "Iteration: 14000. \n",
            " Loss: 1.7072741985321045.\n",
            "  Accuracy: 10.04466551619125\n",
            "\n",
            "Iteration: 14500. \n",
            " Loss: 1.7829086780548096.\n",
            "  Accuracy: 14.886813521469902\n",
            "\n",
            "Iteration: 15000. \n",
            " Loss: 1.9984817504882812.\n",
            "  Accuracy: 9.973606740432443\n",
            "\n",
            "Iteration: 15500. \n",
            " Loss: 1.6488831043243408.\n",
            "  Accuracy: 16.07958582884986\n",
            "\n",
            "Iteration: 16000. \n",
            " Loss: 1.4081838130950928.\n",
            "  Accuracy: 10.009136128311846\n",
            "\n",
            "Iteration: 16500. \n",
            " Loss: 2.2722785472869873.\n",
            "  Accuracy: 9.973606740432443\n",
            "\n",
            "Iteration: 17000. \n",
            " Loss: 1.8532915115356445.\n",
            "  Accuracy: 10.004060501471931\n",
            "\n",
            "Iteration: 17500. \n",
            " Loss: 1.6991866827011108.\n",
            "  Accuracy: 10.115724291950055\n",
            "\n",
            "Iteration: 18000. \n",
            " Loss: 1.9310632944107056.\n",
            "  Accuracy: 9.973606740432443\n",
            "\n",
            "Iteration: 18500. \n",
            " Loss: 2.1150124073028564.\n",
            "  Accuracy: 9.867018576794234\n",
            "\n",
            "Iteration: 19000. \n",
            " Loss: 2.2329344749450684.\n",
            "  Accuracy: 12.678915846106994\n",
            "\n",
            "Iteration: 19500. \n",
            " Loss: 1.5934423208236694.\n",
            "  Accuracy: 10.06496802355091\n",
            "\n",
            "Iteration: 20000. \n",
            " Loss: 1.4468286037445068.\n",
            "  Accuracy: 12.897167800223327\n",
            "\n",
            "Iteration: 20500. \n",
            " Loss: 1.5974528789520264.\n",
            "  Accuracy: 11.27804283829053\n",
            "\n",
            "Iteration: 21000. \n",
            " Loss: 2.024287700653076.\n",
            "  Accuracy: 9.831489188914832\n",
            "\n",
            "Iteration: 21500. \n",
            " Loss: 1.9773002862930298.\n",
            "  Accuracy: 9.93807735255304\n",
            "\n",
            "Iteration: 22000. \n",
            " Loss: 1.8820116519927979.\n",
            "  Accuracy: 11.73992488072277\n",
            "\n",
            "Iteration: 22500. \n",
            " Loss: 1.662401556968689.\n",
            "  Accuracy: 9.795959801035428\n",
            "\n",
            "Iteration: 23000. \n",
            " Loss: 1.5717252492904663.\n",
            "  Accuracy: 9.775657293675769\n",
            "\n",
            "Iteration: 23500. \n",
            " Loss: 2.0889244079589844.\n",
            "  Accuracy: 12.572327682468785\n",
            "\n",
            "Iteration: 24000. \n",
            " Loss: 1.6757726669311523.\n",
            "  Accuracy: 19.01837376916049\n",
            "\n",
            "Iteration: 24500. \n",
            " Loss: 1.8309805393218994.\n",
            "  Accuracy: 16.942442391635367\n",
            "\n",
            "Iteration: 25000. \n",
            " Loss: 1.4595736265182495.\n",
            "  Accuracy: 15.29286366866308\n",
            "\n",
            "Iteration: 25500. \n",
            " Loss: 1.9172214269638062.\n",
            "  Accuracy: 18.06415592325652\n",
            "\n",
            "Iteration: 26000. \n",
            " Loss: 1.4299653768539429.\n",
            "  Accuracy: 14.83098162623084\n",
            "\n",
            "Iteration: 26500. \n",
            " Loss: 2.1362037658691406.\n",
            "  Accuracy: 15.785199472134808\n",
            "\n",
            "Iteration: 27000. \n",
            " Loss: 1.4652001857757568.\n",
            "  Accuracy: 11.151152167292661\n",
            "\n",
            "Iteration: 27500. \n",
            " Loss: 1.7221801280975342.\n",
            "  Accuracy: 23.37326159780733\n",
            "\n",
            "Iteration: 28000. \n",
            " Loss: 2.0658388137817383.\n",
            "  Accuracy: 13.4047304842148\n",
            "\n",
            "Iteration: 28500. \n",
            " Loss: 1.6556739807128906.\n",
            "  Accuracy: 16.99827428687443\n",
            "\n",
            "Iteration: 29000. \n",
            " Loss: 1.8444273471832275.\n",
            "  Accuracy: 13.86661252664704\n",
            "\n",
            "Iteration: 29500. \n",
            " Loss: 1.7574760913848877.\n",
            "  Accuracy: 17.724088924982237\n",
            "\n",
            "Iteration: 30000. \n",
            " Loss: 1.758875846862793.\n",
            "  Accuracy: 21.551111562277942\n",
            "\n",
            "Iteration: 30500. \n",
            " Loss: 1.9113929271697998.\n",
            "  Accuracy: 22.06374987310933\n",
            "\n",
            "Iteration: 31000. \n",
            " Loss: 1.901545524597168.\n",
            "  Accuracy: 19.855852197746422\n",
            "\n",
            "Iteration: 31500. \n",
            " Loss: 1.6230510473251343.\n",
            "  Accuracy: 18.982844381281087\n",
            "\n",
            "Iteration: 32000. \n",
            " Loss: 1.7088210582733154.\n",
            "  Accuracy: 9.628464115318241\n",
            "\n",
            "Iteration: 32500. \n",
            " Loss: 1.8168472051620483.\n",
            "  Accuracy: 21.713531621155212\n",
            "\n",
            "Iteration: 33000. \n",
            " Loss: 1.9512478113174438.\n",
            "  Accuracy: 21.424220891280072\n",
            "\n",
            "Iteration: 33500. \n",
            " Loss: 1.6698576211929321.\n",
            "  Accuracy: 25.291848543295096\n",
            "\n",
            "Iteration: 34000. \n",
            " Loss: 3.2519543170928955.\n",
            "  Accuracy: 13.216932291137956\n",
            "\n",
            "Iteration: 34500. \n",
            " Loss: 1.4986364841461182.\n",
            "  Accuracy: 18.500659831489187\n",
            "\n",
            "Iteration: 35000. \n",
            " Loss: 1.4388043880462646.\n",
            "  Accuracy: 22.08405238046899\n",
            "\n",
            "Iteration: 35500. \n",
            " Loss: 1.7675514221191406.\n",
            "  Accuracy: 14.25236016648056\n",
            "\n",
            "Iteration: 36000. \n",
            " Loss: 1.4571137428283691.\n",
            "  Accuracy: 17.43985382194701\n",
            "\n",
            "Iteration: 36500. \n",
            " Loss: 2.0201010704040527.\n",
            "  Accuracy: 21.403918383920413\n",
            "\n",
            "Iteration: 37000. \n",
            " Loss: 1.9281013011932373.\n",
            "  Accuracy: 22.307379961425237\n",
            "\n",
            "Iteration: 37500. \n",
            " Loss: 0.8539396524429321.\n",
            "  Accuracy: 25.571008019490407\n",
            "\n",
            "Iteration: 38000. \n",
            " Loss: 1.9441754817962646.\n",
            "  Accuracy: 22.35813622982438\n",
            "\n",
            "Iteration: 38500. \n",
            " Loss: 1.3456413745880127.\n",
            "  Accuracy: 24.70307582986499\n",
            "\n",
            "Iteration: 39000. \n",
            " Loss: 1.5083057880401611.\n",
            "  Accuracy: 21.307481473962035\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "CkaUWvnIhWmc",
        "outputId": "69d784ae-a803-4b2a-f07a-1a2001176c06"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print (iteration_loss)\n",
        "plt.plot(iteration_loss)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8fe7b0618926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iteration_loss' is not defined"
          ]
        }
      ]
    }
  ]
}